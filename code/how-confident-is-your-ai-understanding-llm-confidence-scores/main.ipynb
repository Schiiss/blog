{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Probabilities Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogprobsHandler:\n",
    "    \"\"\"\n",
    "    A utility class for handling log probabilities, extracting key names, \n",
    "    and processing structured log probabilities data.\n",
    "    \"\"\"\n",
    "    def prob_to_logprob(self, prob: float) -> float:\n",
    "        \"\"\"\n",
    "        Converts a probability to its logarithmic probability.\n",
    "        \n",
    "        Args:\n",
    "            prob (float): The probability value.\n",
    "        \n",
    "        Returns:\n",
    "            float: The logarithmic probability.\n",
    "        \"\"\"\n",
    "        return np.log(prob)\n",
    "\n",
    "    def logprob_to_prob(self, logprob: float) -> float:\n",
    "        \"\"\"\n",
    "        Converts a logarithmic probability back to a probability.\n",
    "        \n",
    "        Args:\n",
    "            logprob (float): The logarithmic probability.\n",
    "        \n",
    "        Returns:\n",
    "            float: The corresponding probability.\n",
    "        \"\"\"\n",
    "        return np.exp(logprob)\n",
    "\n",
    "    def extract_key_name(self, key: str):\n",
    "        \"\"\"\n",
    "        Extracts the key name from a structured key-value string.\n",
    "        \n",
    "        Args:\n",
    "            key (str): The string containing the key.\n",
    "        \n",
    "        Returns:\n",
    "            str or None: The extracted key name, or None if not found.\n",
    "        \"\"\"\n",
    "        match = re.search(r'([^\"]+)\"\\s*:', key)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def calculate_words_probas(self, logprobs_formatted: List[Dict]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Computes the probabilities of word sequences based on log probabilities.\n",
    "        \n",
    "        Args:\n",
    "            logprobs_formatted (List[Dict]): A list of dictionaries containing tokens and log probabilities.\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[str, float]]: A list of tuples containing token sequences and their corresponding probabilities.\n",
    "        \"\"\"\n",
    "        probas_df = pd.DataFrame({'token': [i['token'] for i in logprobs_formatted],\n",
    "                                  'logprob': [i['logprob'] for i in logprobs_formatted]})\n",
    "\n",
    "        key_value_pairs = []\n",
    "        current_pair = []\n",
    "        for idx, row in probas_df.iterrows():\n",
    "            token = str(row['token'])\n",
    "            if token.strip() != '' and not token.strip() in ['{', '}']:\n",
    "                current_pair.append(idx)\n",
    "            if token.endswith(',\\n') or token.endswith(']\\n') or token.strip().endswith(',') or token.strip().endswith(\n",
    "                    '}') or token.endswith('\"}') or token.endswith(\"'}\") or token.endswith(',\"') or token.endswith(\n",
    "                \",'\"):\n",
    "                if len(current_pair) > 0:\n",
    "                    key_value_pairs.append(current_pair)\n",
    "                current_pair = []\n",
    "        pair_probs = []\n",
    "        for pair in key_value_pairs:\n",
    "            pair_logprob = probas_df.loc[pair, 'logprob'].sum()\n",
    "            pair_prob = self.logprob_to_prob(pair_logprob)\n",
    "            pair_probs.append((''.join(probas_df.loc[pair, 'token']), pair_prob))\n",
    "        return pair_probs\n",
    "\n",
    "    def format_logprobs(self, logprobs) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Formats raw log probability data into a structured dictionary format.\n",
    "        \n",
    "        Args:\n",
    "            logprobs: Raw log probability data.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: A formatted list of dictionaries containing token probabilities.\n",
    "        \"\"\"\n",
    "        logprobs_formatted = []\n",
    "        for logprob in logprobs:\n",
    "            logprob_formatted = {'token': logprob.token, 'logprob': logprob.logprob,\n",
    "                                 'log_topprobs': [{'token': log_topprob.token, 'logprob': log_topprob.logprob}\n",
    "                                                  for log_topprob in logprob.top_logprobs]}\n",
    "            logprobs_formatted.append(logprob_formatted)\n",
    "        return logprobs_formatted\n",
    "\n",
    "    def process_logprobs(self, logprobs_formatted: List[Dict], nested_keys_dct: Dict[str, List[str]] = None):\n",
    "        \"\"\"\n",
    "        Processes formatted log probabilities into structured confidence scores per key field.\n",
    "        \n",
    "        Args:\n",
    "            logprobs_formatted (List[Dict]): A list of formatted log probability dictionaries.\n",
    "            nested_keys_dct (Dict[str, List[str]], optional): A dictionary mapping nested keys to their related values.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary of field names mapped to their aggregated probability scores.\n",
    "        \"\"\"\n",
    "        pair_probs = self.calculate_words_probas(logprobs_formatted)\n",
    "        pair_df = pd.DataFrame(pair_probs, columns=['key_value_pair', 'agg_tokens_proba'])\n",
    "        pair_df['field_name'] = pair_df['key_value_pair'].apply(self.extract_key_name)\n",
    "        pair_df = pair_df[pair_df['field_name'].notna()]\n",
    "\n",
    "        if nested_keys_dct is not None:\n",
    "            for nested_key_name, nested_key_values in nested_keys_dct.items():\n",
    "                nested_key_str = '|'.join(nested_key_values)\n",
    "                nested_rows = pair_df[pair_df['field_name'].str.contains(nested_key_str, case=False)]\n",
    "                if len(nested_rows) > 0:\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'key_value_pair': [' '.join(nested_rows['key_value_pair'])],\n",
    "                        'agg_tokens_proba': [nested_rows['agg_tokens_proba'].prod()],\n",
    "                        'field_name': [nested_key_name]\n",
    "                    })\n",
    "                    pair_df = pd.concat([pair_df, new_row], axis=0, ignore_index=True)\n",
    "        pair_df['agg_tokens_proba'] = pair_df['agg_tokens_proba'].round(4)\n",
    "\n",
    "        fields_llm_confidences = dict(pair_df.set_index('field_name')['agg_tokens_proba'].to_dict())\n",
    "        return fields_llm_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_handler = LogprobsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step: How does the log probabilities class work? \n",
    "\n",
    "In the steps below we dissect the class step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Format API Response\n",
    "\n",
    "The first function to be called from the LogprobsHandler() class is format_logprobs. This will format the raw API response probability data into a structured dictionary format.\n",
    "\n",
    "Note, the output lloks something like this:\n",
    "\n",
    "[{'token': '{\"', 'logprob': -1.9361265e-07, 'log_topprobs': []}, {'token': 'name', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\":\"', 'logprob': 0.0, 'log_topprobs': []}, {'token': 'Science', 'logprob': -0.008698363, 'log_topprobs': []}, {'token': ' Fair', 'logprob': -6.392203e-06, 'log_topprobs': []}, {'token': '\",\"', 'logprob': -0.034407083, 'log_topprobs': []}, {'token': 'date', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\":\"', 'logprob': 0.0, 'log_topprobs': []}, {'token': '202', 'logprob': -0.0017659782, 'log_topprobs': []}, {'token': '3', 'logprob': -0.0001307143, 'log_topprobs': []}, {'token': '-', 'logprob': 0.0, 'log_topprobs': []}, {'token': '10', 'logprob': -0.03874472, 'log_topprobs': []}, {'token': '-', 'logprob': 0.0, 'log_topprobs': []}, {'token': '06', 'logprob': -1.0780874, 'log_topprobs': []}, {'token': '\",\"', 'logprob': -9.6629374e-05, 'log_topprobs': []}, {'token': 'participants', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\":[\"', 'logprob': -9.0883464e-07, 'log_topprobs': []}, {'token': 'Alice', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\",\"', 'logprob': -3.1281633e-07, 'log_topprobs': []}, {'token': 'Bob', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\"]', 'logprob': -5.5122365e-07, 'log_topprobs': []}, {'token': '}', 'logprob': -1.9361265e-07, 'log_topprobs': []}]\n",
    "\n",
    "All of the words are seperated in the form of 'tokens' and we will need to process the data to get the confidence scores for each of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '{\"', 'logprob': -1.9361265e-07, 'log_topprobs': []}, {'token': 'name', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\":\"', 'logprob': 0.0, 'log_topprobs': []}, {'token': 'Science', 'logprob': -0.008698363, 'log_topprobs': []}, {'token': ' Fair', 'logprob': -6.392203e-06, 'log_topprobs': []}, {'token': '\",\"', 'logprob': -0.034407083, 'log_topprobs': []}, {'token': 'date', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\":\"', 'logprob': 0.0, 'log_topprobs': []}, {'token': '202', 'logprob': -0.0017659782, 'log_topprobs': []}, {'token': '3', 'logprob': -0.0001307143, 'log_topprobs': []}, {'token': '-', 'logprob': 0.0, 'log_topprobs': []}, {'token': '10', 'logprob': -0.03874472, 'log_topprobs': []}, {'token': '-', 'logprob': 0.0, 'log_topprobs': []}, {'token': '06', 'logprob': -1.0780874, 'log_topprobs': []}, {'token': '\",\"', 'logprob': -9.6629374e-05, 'log_topprobs': []}, {'token': 'participants', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\":[\"', 'logprob': -9.0883464e-07, 'log_topprobs': []}, {'token': 'Alice', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\",\"', 'logprob': -3.1281633e-07, 'log_topprobs': []}, {'token': 'Bob', 'logprob': 0.0, 'log_topprobs': []}, {'token': '\"]', 'logprob': -5.5122365e-07, 'log_topprobs': []}, {'token': '}', 'logprob': -1.9361265e-07, 'log_topprobs': []}]\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.environ.get(\"OPENAI_ENDPOINT\"), \n",
    "  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "  azure_deployment=os.environ.get(\"OPENAI_MODEL\"),\n",
    "  api_version=\"2024-10-21\"\n",
    ")\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "    \n",
    "\n",
    "response_raw = client.beta.chat.completions.parse(messages=[{'role': 'user', 'content': 'Alice and Bob are going to a science fair on Friday.'}], model=os.environ.get(\"OPENAI_MODEL\"), logprobs=True, response_format=CalendarEvent)\n",
    "\n",
    "response_logprobs = response_raw.choices[0].logprobs.content if hasattr(response_raw.choices[0], 'logprobs') else []\n",
    "\n",
    "logprobs_formatted = logprobs_handler.format_logprobs(response_logprobs)\n",
    "\n",
    "print(logprobs_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process the log probs\n",
    "\n",
    "To process the log probs we need to run a few functions:\n",
    "\n",
    "-\tcalculate_words_probas: Computes the probabilities of word sequences based on log probabilities.\n",
    "\n",
    "-\textract_key_name: Extracts the key name from a structured key-value string using a regex statement.\n",
    "\n",
    "As you can see below, calling these two functions outputs the data into a nice data frame.\n",
    "\n",
    "Note, the output will look something like this:\n",
    "\n",
    "| key_value_pair                    | agg_tokens_proba | field_name   |\n",
    "|------------------------------------|-----------------|-------------|\n",
    "| {\"name\":\"Science Fair\",\"          | 0.957804        | name        |\n",
    "| date\":\"2023-10-06\",\"               | 0.326663        | date        |\n",
    "| participants\":[\"Alice\",\"           | 0.999999        | participants |\n",
    "\n",
    "Pretty high confidence score for the name and participants fields, but not so much for the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_value_pair</th>\n",
       "      <th>agg_tokens_proba</th>\n",
       "      <th>field_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"name\":\"Science Fair\",\"</td>\n",
       "      <td>0.957804</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date\":\"2023-10-06\",\"</td>\n",
       "      <td>0.326663</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>participants\":[\"Alice\",\"</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>participants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             key_value_pair  agg_tokens_proba    field_name\n",
       "0  {\"name\":\"Science Fair\",\"          0.957804          name\n",
       "1      date\":\"2023-10-06\",\"          0.326663          date\n",
       "2  participants\":[\"Alice\",\"          0.999999  participants"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair_probs = logprobs_handler.calculate_words_probas(logprobs_formatted)\n",
    "pair_df = pd.DataFrame(pair_probs, columns=['key_value_pair', 'agg_tokens_proba'])\n",
    "pair_df['field_name'] = pair_df['key_value_pair'].apply(logprobs_handler.extract_key_name)\n",
    "pair_df = pair_df[pair_df['field_name'].notna()]\n",
    "display(pair_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all Together\n",
    "\n",
    "Below will run the example end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParsedChatCompletion[CalendarEvent](id='chatcmpl-B0dQv4id9nFEIRxI6Uc88jqFUAHO6', choices=[ParsedChoice[CalendarEvent](finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='{\"', bytes=[123, 34], logprob=-1.9361265e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='name', bytes=[110, 97, 109, 101], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\":\"', bytes=[34, 58, 34], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='Science', bytes=[83, 99, 105, 101, 110, 99, 101], logprob=-0.010148544, top_logprobs=[]), ChatCompletionTokenLogprob(token=' Fair', bytes=[32, 70, 97, 105, 114], logprob=-6.392203e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token='\",\"', bytes=[34, 44, 34], logprob=-0.042743146, top_logprobs=[]), ChatCompletionTokenLogprob(token='date', bytes=[100, 97, 116, 101], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\":\"', bytes=[34, 58, 34], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='202', bytes=[50, 48, 50], logprob=-0.0019719347, top_logprobs=[]), ChatCompletionTokenLogprob(token='3', bytes=[51], logprob=-0.0001307143, top_logprobs=[]), ChatCompletionTokenLogprob(token='-', bytes=[45], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='10', bytes=[49, 48], logprob=-0.055911675, top_logprobs=[]), ChatCompletionTokenLogprob(token='-', bytes=[45], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='06', bytes=[48, 54], logprob=-1.0992861, top_logprobs=[]), ChatCompletionTokenLogprob(token='\",\"', bytes=[34, 44, 34], logprob=-0.00010938417, top_logprobs=[]), ChatCompletionTokenLogprob(token='participants', bytes=[112, 97, 114, 116, 105, 99, 105, 112, 97, 110, 116, 115], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\":[\"', bytes=[34, 58, 91, 34], logprob=-1.147242e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token='Alice', bytes=[65, 108, 105, 99, 101], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\",\"', bytes=[34, 44, 34], logprob=-3.1281633e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='Bob', bytes=[66, 111, 98], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\"]', bytes=[34, 93], logprob=-6.704273e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='}', bytes=[125], logprob=-1.9361265e-07, top_logprobs=[])], refusal=None), message=ParsedChatCompletionMessage[CalendarEvent](content='{\"name\":\"Science Fair\",\"date\":\"2023-10-06\",\"participants\":[\"Alice\",\"Bob\"]}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=CalendarEvent(name='Science Fair', date='2023-10-06', participants=['Alice', 'Bob'])), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1739491281, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_b045b4af17', usage=CompletionUsage(completion_tokens=22, prompt_tokens=87, total_tokens=109, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "{\"name\":\"Science Fair\",\"date\":\"2023-10-06\",\"participants\":[\"Alice\",\"Bob\"]}\n",
      "{'name': 0.9485, 'date': 0.3143, 'participants': 1.0}\n",
      "Extracted data with confidence scores saved to output_data_with_confidence.csv\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.environ.get(\"OPENAI_ENDPOINT\"), \n",
    "  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "  azure_deployment=os.environ.get(\"OPENAI_MODEL\"),\n",
    "  api_version=\"2024-10-21\"\n",
    ")\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "    \n",
    "\n",
    "response_raw = client.beta.chat.completions.parse(messages=[{'role': 'user', 'content': 'Alice and Bob are going to a science fair on Friday.'}], model=os.environ.get(\"OPENAI_MODEL\"), logprobs=True, response_format=CalendarEvent)\n",
    "\n",
    "print(response_raw)\n",
    "\n",
    "print(response_raw.choices[0].message.content)\n",
    "\n",
    "response_logprobs = response_raw.choices[0].logprobs.content if hasattr(response_raw.choices[0], 'logprobs') else []\n",
    "\n",
    "logprobs_formatted = logprobs_handler.format_logprobs(response_logprobs)\n",
    "\n",
    "confidence = logprobs_handler.process_logprobs(logprobs_formatted)\n",
    "\n",
    "print(confidence)\n",
    "\n",
    "data = json.loads(response_raw.choices[0].message.content)\n",
    "csv_file = \"output_data_with_confidence.csv\"\n",
    "\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    writer.writerow([\"Field\", \"Value\", \"Confidence Score\"])\n",
    "    for field, value in data.items():\n",
    "        confidence_score = confidence.get(field, 0) * 100 \n",
    "        writer.writerow([field, json.dumps(value) if isinstance(value, list) else value, f\"{confidence_score:.2f}%\"])\n",
    "\n",
    "print(f\"Extracted data with confidence scores saved to {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
